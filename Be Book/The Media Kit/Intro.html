<HTML><HEAD><TITLE>The Media Kit: The Media Kit</TITLE></HEAD><BODY BGCOLOR=#ffffff>


<!--TOP LINKS-->
<HR>
<center>
<TABLE bgcolor=ddeeee>
<TR><TD>
<A HREF="index.html"><FONT face=HELVETICA>The Media Kit Table of Contents</FONT></A>&nbsp;&nbsp;
<TD>
&nbsp;&nbsp;<A HREF="The%20Media%20Kit%20Master%20Index.html"><FONT face=HELVETICA>The Media Kit Index</FONT></A>
</TABLE></center>
<!--TOP LINKS-->


<HR>
<H1>
<A NAME="The%20Media%20Kit"></A>The Media Kit
</H1>
<P>
The Media Kit provides powerful support for all forms of media (including, but not limited to, audio and video), including both playback and recording from and to a wide variety of media and devices.
<P>
There are two levels of Media Kit programming: the high level, where an application accesses the Media Kit to play and record sound and video, and the low level, which involves actually creating the nodes that manipulate media data.
<P>
<HR>
<H2>
<A NAME="Media%20Kit%20Concepts"></A><FONT SIZE=6>M</FONT>edia <FONT SIZE=6>K</FONT>it <FONT SIZE=6>C</FONT>oncepts
</H2>
<P>
This section is a general overview of key Media Kit concepts.
<P>
<HR>
<H3>
<A NAME="Nodes"></A>Nodes
</H3>
<P>
The first thing that you need to understand as a media programmer working with the BeOS is the concept of a <B>node</B>.  Generically speaking, a node is a specialized object in the media system that processes buffers of media data.  All nodes are indirectly derived from the <A HREF="MediaNode.html#BMediaNode">BMediaNode</A> class (but never directly from BMediaNode; instead, nodes are derived from specialized node type classes).
<P>
Nodes can be loaded from add-on modules, or they can be created within an application itself.  See the <A HREF="MediaAddOn.html#BMediaAddOn">BMediaAddOn</A> class for details.
<P>
The <B>node kind</B> (defined by the <B><TT><A HREF="MediaNode.html#node_kind">node_kind</A></TT></B> type) is a description of the basic capabilities of a node.  &nbsp;<a href="#See%20Types%20of%20Nodes%20on%20page4.">See "Types of Nodes" on page4.</a>
<P>
A <B><TT><A HREF="misc_api.html#media_node">media_node</A></TT></B> is a structure that an application uses when working with media nodes; interactions with the Media Roster are almost always done using this structure instead of an object actually derived from BMediaNode.  The reason for this that applications will often need to share nodes, and due to protected memory, it's not really feasible to pass a <A HREF="MediaNode.html#BMediaNode">BMediaNode</A> pointer among the applications.
<P>
<TABLE  CELLPADDING=4>
	<TR>
		<TD>&nbsp;
		<TD>
			<HR NOSHADE>
			<TABLE CELLPADDING=4>
				<TR>
					<TD VALIGN=TOP>
						<IMG SRC="../art/infoBullet.gif">
					<TD><FONT FACE="helvetica">
No portion of the media node protocol is optional; if you don't follow all the rules precisely, you risk hurting media performance, and since BeOS is the Media OS, that would be a bad thing.
<P>
</UL>
<P>
For more detailed information about the architecture of the Media Kit (in particular, how nodes relate to one another), please see the <A HREF="MediaNode.html#BMediaNode">BMediaNode</A> class.
<P>
<A NAME="Types%20of%20Nodes"></A>
<P>
<H4>
<A NAME="Types%20of%20Nodes"></A>Types of Nodes
</H4>
<P>
There are several basic kinds of nodes.  Each of them is derived originally from the <A HREF="MediaNode.html#BMediaNode">BMediaNode</A> class.  Any nodes that you might implement will be derived, in turn, from one or more of these node types.  The <B>node kind</B> indicates which of these types a node implements.
<P>
<H5>
<A NAME="Producers"></A>Producers
</H5>
<P>
A producer (a node derived from the <A HREF="BufferProducer.html#BBufferProducer">BBufferProducer</A> class) outputs media buffers, which are then received by consumers.  A producer might be generating nodes on its own (for example, a tone generator might be using a mathematical formula to generate a sound, or an audio file player might be loading data from disk and sending buffers containing its audio data).  Other producers might be responsible for acquiring data from media hardware (such as a video camera) and passing the media buffers to consumers down the line.
<P>
In the example in the <a href="#Communicating%20With%20Nodes">"Communicating With Nodes"</a> section, the sound file reader node is an example of a producer.
<P>
<H5>
<A NAME="Consumers"></A>Consumers
</H5>
<P>
Consumers (nodes derived from BBufferConsumer), receive buffers from a producer and process them in some manner.  For example, a sound card's software would provide a consumer node that receives audio buffers and plays them through the card's hardware.
<P>
In the example in the <a href="#Communicating%20With%20Nodes">"Communicating With Nodes"</a> section, the sound player node is an example of a consumer.
<P>
<H5>
<A NAME="Consumer/Producers%20(Filters)"></A>Consumer/Producers (Filters)
</H5>
<P>
A consumer/producer (a node that derives from both <A HREF="BufferConsumer.html#BBufferConsumer">BBufferConsumer</A> and BBufferProducer) is also called a <B>filter</B>.  A filter accepts buffers (like a consumer), processes them in some manner, then sends them back out again (like a producer).  This can be used to alter sound or video data.
<P>
For example, an audio filter might add a reverb effect to sound buffers, or a video filter might add captioning to a video stream.
<P>
In the example in the <a href="#Communicating%20With%20Nodes">"Communicating With Nodes"</a> section, the equalizer node is an example of a filter node.
<P>
<H5>
<A NAME="Controllables"></A>Controllables
</H5>
<P>
If a node wishes to provide the user options for configuring its functionality, the node can derive from BControllable.  This provides features for creating a network of controllable parameters, and for publishing this information to Media Kit-savvy applications (including the Media preference applications).
<P>
<H5>
<A NAME="Time%20Sources"></A>Time Sources
</H5>
<P>
A time source node broadcasts timing information that can be used by other nodes.  All nodes are slaved to a time source, which provides synchronization among all nodes slaved to that time source.  Typically, applications won't need to worry about this, because any node created through the <A HREF="MediaRoster.html#BMediaRoster">BMediaRoster</A> class are automatically slaved to the system (default) time source.
<P>
<TABLE  CELLPADDING=4>
	<TR>
		<TD>&nbsp;
		<TD>
			<HR NOSHADE>
			<TABLE CELLPADDING=4>
				<TR>
					<TD VALIGN=TOP>
						<IMG SRC="../art/infoBullet.gif">
					<TD><FONT FACE="helvetica">
A node can be derived from any of these types (BBufferProducer, BBufferConsumer, <A HREF="TimeSource.html#BTimeSource">BTimeSource</A>, and BControllable), in any combination, as appropriate.
<P>
						</FONT>			
			</TABLE>
			<HR NOSHADE>
</TABLE>

<P>
<H4>
<A NAME="The%20Node%20Kind"></A>The Node Kind
</H4>
<P>
The <B><TT><A HREF="MediaNode.html#node_kind">node_kind</A></TT></B> type is used to identify which of these interfaces a node implements; this lets the Media Kit know which APIs your node supports.  These flags include <B><TT><A HREF="MediaNode.html#B_BUFFER_PRODUCER">B_BUFFER_PRODUCER</A></TT></B>, <B><TT><A HREF="MediaNode.html#B_BUFFER_CONSUMER">B_BUFFER_CONSUMER</A></TT></B>, <B><TT><A HREF="MediaNode.html#B_TIME_SOURCE">B_TIME_SOURCE</A></TT></B>, and <B><TT><A HREF="MediaNode.html#B_FILE_INTERFACE">B_FILE_INTERFACE</A></TT></B>.
<P>
There are other flags available in the node kind; these indicate special node functions supported by the node.  These are <B><TT><A HREF="MediaNode.html#B_PHYSICAL_INPUT">B_PHYSICAL_INPUT</A></TT></B>, which indicates that the node is a physical input device, <B><TT><A HREF="MediaNode.html#B_PHYSICAL_OUTPUT">B_PHYSICAL_OUTPUT</A></TT></B>, which indicates that the device is a physical output device, and <B><TT><A HREF="MediaNode.html#B_SYSTEM_MIXER">B_SYSTEM_MIXER</A></TT></B>, which indicates that the node is the system mixer.
<P>
The primary purpose to these flags is to help user interfaces determine how to draw the interface for these nodes; they get special icons in the Media preference application, for example.
<P>
<H4>
<A NAME="Deriving%20From%20Multiple%20Classes"></A>Deriving From Multiple Classes
</H4>
<P>
For example, if you're creating a sound card node that plays audio to stereo speakers, you would need to derive from <A HREF="BufferConsumer.html#BBufferConsumer">BBufferConsumer</A> (in order to receive audio buffers).  You could also derive from <A HREF="TimeSource.html#BTimeSource">BTimeSource</A> if your sound card has the ability to provide timing information to others.  And if you want the user to be able to control the volume, balance, and so forth, you would also derive from BControllable.
<P>
If your sound card also provides a digitizer input, you would actually create a second node to support that feature.  It would inherit from <A HREF="BufferProducer.html#BBufferProducer">BBufferProducer</A> (so it can generate audio buffers for other nodes to use).  It might also derive from <A HREF="TimeSource.html#BTimeSource">BTimeSource</A> and BControllable.
<P>
But not all nodes necessarily represent a physical hardware device.  If you want to create a filter&mdash;for example, a noise-reduction filter&mdash;you can create a node to do this too.  Simply derive from both <A HREF="BufferConsumer.html#BBufferConsumer">BBufferConsumer</A> (so you can receive buffers) and <A HREF="BufferProducer.html#BBufferProducer">BBufferProducer</A> (so you can send back out the altered buffers).
<P>
<HR>
<H3>
<A NAME="Source%20&amp;%20Destination%20vs.%20Output%20&amp;%20Input"></A>Source &amp; Destination vs. Output &amp; Input
</H3>
<P>
Beginning Media Kit programmers may have trouble understanding the difference between a <B><TT><A HREF="misc_api.html#media_source">media_source</A></TT></B> and a <B><TT><A HREF="misc_api.html#media_output">media_output</A></TT></B>, and a <B><TT><A HREF="misc_api.html#media_destination">media_destination</A></TT></B> and a <B><TT><A HREF="misc_api.html#media_input">media_input</A></TT></B>.
<P>
The <B><TT><A HREF="misc_api.html#media_source">media_source</A></TT></B> and <B><TT><A HREF="misc_api.html#media_destination">media_destination</A></TT></B> structures describe a "socket" of sorts (much like in networking).  These are the ends of the connection, much like the jacks you might plug cables into to connect various components of a stereo system.  They're relatively small, lightweight descriptions containing only the information needed during real-time manipulation of nodes.  Buffers travel from the source to the destination.  You can use the basic operators (=, ==, and !=) on these objects.
<P>
<B><TT>media_source::null</TT></B> and <B><TT>media_destination::null</TT></B> represent uninitialized endpoints.
<P>
The <B><TT><A HREF="misc_api.html#media_output">media_output</A></TT></B> and <B><TT><A HREF="misc_api.html#media_input">media_input</A></TT></B> structures describe an actual connection between a <B><TT><A HREF="misc_api.html#media_source">media_source</A></TT></B> and a <B><TT><A HREF="misc_api.html#media_destination">media_destination</A></TT></B>, including the source and destination, the connetion's name, and the format of the data the connection is intended to handle.  These are larger, and contain additional information needed when presenting a user interface describing the connections between nodes.
<P>
The <B><TT><A HREF="misc_api.html#media_input">media_input</A></TT></B> structure describes the receiving (upstream) end of a connection; <B><TT><A HREF="misc_api.html#media_output">media_output</A></TT></B> describes the sender (the downstream end).
<P>
Although <B><TT><A HREF="misc_api.html#media_output">media_output</A></TT></B> and <B><TT><A HREF="misc_api.html#media_input">media_input</A></TT></B> contain all the information of the <B><TT><A HREF="misc_api.html#media_source">media_source</A></TT></B> and <B><TT><A HREF="misc_api.html#media_destination">media_destination</A></TT></B> structures, the latter structures exist because when you're doing real-time manipulation of media data, you don't want to be tossing large blocks of data around unless you have to.  And you don't have to.
<P>
<HR>
<H3>
<A NAME="Media%20Formats"></A>Media Formats
</H3>
<P>
The media_format structure describes the type of media being passed through a connection.  The application and the nodes with which it's working negotiate the format through a sequence of calls and callbacks.
<P>
This structure contains a basic media type (such as <B><TT>B_MEDIA_RAW_AUDIO</TT></B> or <B><TT>B_MEDIA_ENCODED_VIDEO</TT></B>) and a union containing additional information depending on the basic type.
<P>
<H4>
<A NAME="Raw%20Audio"></A>Raw Audio
</H4>
<P>
The <B><TT><A HREF="misc_api.html#media_format">media_format</A></TT></B> structure describing <B><TT>B_MEDIA_RAW_AUDIO</TT></B> media contains the following information:
<P>
<UL>
<LI>Frame rate: the number of frames of audio per second.
<P>
<LI>Format: the data type of each audio sample.
<P>
<LI>Channel count: the number of samples per frame.  For example, stereo sound has two channels (left and right).
<P>
<LI>Byte order: indicates whether the data is big-endian or little-endian.
<P>
<LI>Buffer size: the number of bytes of data per buffer.
<P>
</UL>
<P>
<H4>
<A NAME="Raw%20Video"></A>Raw Video
</H4>
<P>
The <B><TT><A HREF="misc_api.html#media_format">media_format</A></TT></B> structure describing <B><TT>B_MEDIA_RAW_VIDEO</TT></B> media contains the following information:
<P>
<UL>
<LI>Field rate: the number of fields (buffers) of video per second.
<P>
<LI>Interlace: the number of fields per frame.
<P>
<LI>Display information: the color space, line width, line count, bytes per row, and so forth.
<P>
</UL>
<P>
<H4>
<A NAME="Format%20Wildcards"></A>Format Wildcards
</H4>
<P>
A format wildcard indicates unspecified parts of the format.  This is used during negotiation so a node or application can say "I don't care about this partifular part of the format; I'm flexible."  For example, if your application is negotiating with a video node, and you can handle any bit depth of video, you would specify a wildcard in the color space field of the format.
<P>
Each format type has one wildcard object.
<P>
For example, if you're preparing to negotiate with an audio node, and don't care about the frame rate, the following code will set up the <B><TT><A HREF="misc_api.html#media_format">media_format</A></TT></B> structure for negotiation:
<P>
<PRE>&nbsp;&nbsp;&nbsp;media_format format;
&nbsp;&nbsp;&nbsp;media_raw_audio_format wc;
&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;wc = media_raw_audio_format::wildcard;
&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;format.type = B_MEDIA_RAW_AUDIO;
&nbsp;&nbsp;&nbsp;format.u.raw_audio.frame_rate = wc.frame_rate;</PRE>
<P>
<H4>
<A NAME="Audio:%20Samples%20and%20Frames%20and%20Buffers%20(Oh%20my)"></A>Audio: Samples and Frames and Buffers (Oh my)
</H4>
<P>
A <I>sample</I> is a single value that defines the amplitude of an audio waveform at a given instant in time.  This value can be represented in a number of ways: as a one-byte integer, as a two-byte integer, as a floating-point value, or in other ways.  The native audio sample format in BeOS is floating-point, where a value of 0.0 represents a wave point with no amplitude.  Positive values indicate a sample whose amplitude is above the zero point, and negative values represent samples below the zero point.
<P>
A <I>frame</I> is the set of samples that describes a sound at a given instant in time.  If the audio contains multiple channels, a frame contains multiple samples, one for each channel.  A monaural audio stream contains one sample per frame, while a stereo audio stream contains two samples per frame.  Surround sound formats contain even more samples per frame of audio.
<P>
A <I>buffer</I> is a more efficient way of sending audio from one node to another.  Instead of beaming around thousands of individual frames, frames are grouped into buffers that are then transferred <I>en masse</I> along the chain of nodes.  This improves throughput and reduces overhead.
<P>
See &nbsp;<a href="misc_api.html#media_raw_audio_format%20on%20page347">"media_raw_audio_format" on page347</a> for more information about describing audio formats.
<P>
<H4>
<A NAME="Video:%20Interlacing,%20Fields,%20and%20Frames"></A>Video: Interlacing, Fields, and Frames
</H4>
<P>
Video can be represented either as non-interlaced or interlaced data.  Both NTSC video (the standard format for television video in the United States) and PAL video (the standard in many other countries) are interlaced.
<P>
Typically, a video stream will have either one or two fields per frame.  If the video isn't interlaced, there's only one field, which contains all the scan lines in the video stream.  If the video is interlaced, there are usually two fields per frame (it might be more, but two is most common).  One field contains the even lines of the frame, the other contains the odd lines.
<P>
Video buffers in the BeOS Media Kit always contain either a complete frame or a complete field; buffers never contain partial frames or fields.
<P>
See &nbsp;<a href="misc_api.html#media_raw_video_format%20on%20page348">"media_raw_video_format" on page348</a> for more information about describing the format of video data.
<P>
<H4>
<A NAME="Reference%20Material"></A>Reference Material
</H4>
<P>
This chapter doesn't pretend to be a tutorial on the intricacies of audio and video data formats.  There are plenty of good reference books on these subjects.  Here's a list of books our engineers suggest:
<P>
<UL>
<LI><B>The Art of Digital Audio</B> by John Watkinson (ISBN 0240513207)
<P>
<LI><B>Compression in Audio and Video</B> by John Watkinson (ISBN 0240513940)
<P>
<LI><B>Digital Audio Signal Processing</B> by Udo Zolzer (ISBN 0471972266)
<P>
<LI><B>Introduction to Signal Processing</B> by Sophocles Orphanidis (ISBN )
<P>
<LI><B>Principles of Digital Audio</B> by Ken C. Pohlmann (ISBN 0070504695)
<P>
<LI><B>A Programmer's Guide to Sound</B> by Tim Kientzle (ISBN 0201419726)
<P>
<LI><B>Video Demystified</B> by Keith Jack (ISBN 18780723X)
<P>
</UL>
<P>
<HR>
<H3>
<A NAME="Buffers"></A>Buffers
</H3>
<P>
A <A HREF="Buffer.html#BBuffer">BBuffer</A> is a packet of media data that can be passed between nodes.  It contains a header that describes the data and the actual data being transmitted.
<P>
The buffer header contains inforrmation that tells you what the data is:
<P>
<UL>
<LI>Start time: the time at which the data should be performed.
<P>
<LI>Size used: the number of valid data bytes in the buffer.
<P>
<LI>Additional information based on the media type.
<P>
</UL>
<P>
For optimal performance, the media system stores buffer data in shared memory areas so that different applications and nodes can access the same buffer, without having to copy it around in memory.  Buffers are passed from one node to the next by giving each node a <A HREF="Buffer.html#BBuffer">BBuffer</A> that references the same area of memory.
<P>
<A NAME="Communicating%20With%20Nodes"></A>
<P>
<HR>
<H3>
<A NAME="Communicating%20With%20Nodes"></A>Communicating With Nodes
</H3>
<P>
The <A HREF="MediaRoster.html#BMediaRoster">BMediaRoster</A> class provides the application interface to all available nodes (whether they're created by the application or by an add-on).  An application instantiates the nodes it needs, then establishes the connections between them that will accomplish the desired task.
<P>
For example, if an application wants to play a sound file through a graphic equalizer, it might first instantiate a node that reads sound data from a disk file and outputs audio buffers, then instantiate a node that performs filtering on audio buffers, and finally, a node that plays sound buffers to the speakers.
<P>
Once these three nodes are instantiated, the application creates the links between them.  The output of the audio file reading node is connected to the input of the equalizer node, then the equalizer node's output is connected to the sound player node's input.:
<P>
<IMG SRC="art/Intro1.GIF" ALIGN="bottom">
<P>
Once these connections are established, the application can then begin playing the sound file, by telling the first node what sound file to play, and then starting all of the nodes running.  The sound file reader creates buffers containing audio data, and passes those along to the equalizer node, which alters them, then passes them along to the sound player node, which plays the buffers and recycles them for reuse.  A more detailed example of how to work with nodes to play back media data is given in the <A HREF="MediaRoster.html#BMediaRoster">BMediaRoster</A> class.
<P>
<H4>
<A NAME="Connections%20at%20the%20Low%20Level"></A>Connections at the Low Level
</H4>
<P>
Each media node maintains a <B>control port</B>.  The Media Kit interacts with the node by sending messages to the node's control port.
<P>
<HR>
<H3>
<A NAME="Time"></A>Time
</H3>
<P>
One of the most important issues in manipulating media data is to properly track and synchronize precisely-timed events.  There are several kinds of time (and you don't even have to be Stephen Hawking to understand them):
<P>
<H4>
<A NAME="Media%20Time"></A>Media Time
</H4>
<P>
Media time is time relative to a particular media file, and is represented in terms of microseconds since the beginning of the file.  Seek operations are performed using media time.
<P>
<H4>
<A NAME="Real%20Time"></A>Real Time
</H4>
<P>
Real time is the time reported by the system clock, and is represented in terms of microseconds since the computer was booted.  It's used in conjunction with system calls, such as <B><TT><A HREF="../The%20Kernel%20Kit/Threads.html#snooze()">snooze()</A></TT></B>, <B><TT><A HREF="../The%20Kernel%20Kit/Ports.html#read_port_etc()">read_port_etc()</A></TT></B>, and so forth.
<P>
<H4>
<A NAME="Performance%20Time"></A>Performance Time
</H4>
<P>
Performance time is the time at which events should occur at the final output.  It's reported by a time source node, which establishes the relationship between real time and performance time.
<P>
Usually a single time source is the master clock for all the nodes in a node chain.
<P>
<H4>
<A NAME="Performance%20Time%20vs.%20Real%20Time"></A>Performance Time vs. Real Time
</H4>
<P>
There are two reasons why performance time and real time may differ: if the master clock isn't the system time, there may be drift over time.  Also, latency caused by the time it takes for data to pass from one node to another can cause drift as well.
<P>
<HR>
<H3>
<A NAME="Latency"></A>Latency
</H3>
<P>
<B>Latency</B> is the amount of time it takes to do something.  There are two key types of latency:
<P>
<UL>
<LI><B>Internal (or processing) latency</B> is the time it takes for a node to process a buffer.  This doesn't include transmission time.
<P>
<LI><B>Downstream latency</B> is the amount of time that passes between the time a node sends a buffer to the time at which the buffer can be performed.
<P>
</UL>
<P>
<H4>
<A NAME="A%20Latency%20Example"></A>A Latency Example
</H4>
<P>
Let's consider a case in which three nodes are connected.  The first node has a processing latency of 3 microseconds, the second has a processing latency of 2 microseconds, and the last has a processing latency of 1 microsecond.
<P>
<TABLE  CELLPADDING=4>
	<TR>
		<TD>&nbsp;
		<TD>
			<HR NOSHADE>
			<TABLE CELLPADDING=4>
				<TR>
					<TD VALIGN=TOP>
						<IMG SRC="../art/infoBullet.gif">
					<TD><FONT FACE="helvetica">
This example uses the term "microseconds" because the Media Kit measures time in microseconds; however, the latencies used in this example may not be indicative of a real system.
<P>
						</FONT>			
			</TABLE>
			<HR NOSHADE>
</TABLE>

<P>
In addition, 2 microseconds is required for buffers to pass from one node to the next.  The total latency of this chain of nodes, then, is 3 + 2 + 2 + 2 + 1 = 10 microseconds.
<P>
A buffer is scheduled to be played at a performance time of 50 microseconds.  In order to get this buffer to the last node in time to be played at the right time, it needs to begin being processed at 40 microseconds.  We see this in the diagram below.
<P>
<IMG SRC="art/latency1.gif" ALIGN="bottom">
<P>
After the buffer has been processed by Node 1 and has been passed along to Node 2, 5 microseconds have passed.  Node 2 will take 2 microseconds to process the buffer.  This gets us to a performance time of 45 microseconds:
<P>
<IMG SRC="art/latency2.gif" ALIGN="bottom">
<P>
Node 2 processes the buffer, and passes it along to Node 3.  This takes a total of 4 microseconds (2 microseconds of processing time plus 2 microseconds to be sent to Node 3).  We arrive at the performance time of 49 microseconds:
<P>
<IMG SRC="art/latency3.gif" ALIGN="bottom">
<P>
Finally, Node 3 processes the buffer; this requires 1 microsecond of processing time.  At this point, the performance time of 50 microseconds has been reached, and the buffer has been performed on schedule.
<P>
<IMG SRC="art/latency4.gif" ALIGN="bottom">
<P>
<HR>
<H2>
<A NAME="Using%20the%20Media%20Kit"></A><FONT SIZE=6>U</FONT>sing the <FONT SIZE=6>M</FONT>edia <FONT SIZE=6>K</FONT>it
</H2>
<P>
If you're writing an application that wants to record or play back some form of media data (such as a sound or a video file), all your media needs are served by the <A HREF="MediaRoster.html#BMediaRoster">BMediaRoster</A> class.  This class provides access to the various nodes, and lets you establish the relationships among them that are necessary to perform the tasks you'd like to accomplish.
<P>
<TABLE  CELLPADDING=4>
	<TR>
		<TD>&nbsp;
		<TD>
			<HR NOSHADE>
			<TABLE CELLPADDING=4>
				<TR>
					<TD VALIGN=TOP>
						<IMG SRC="../art/infoBullet.gif">
					<TD><FONT FACE="helvetica">
The <A HREF="MediaNode.html#BMediaNode">BMediaNode</A> is an abstract class; you don't call its functions directly.  Instead, you use <A HREF="MediaRoster.html#BMediaRoster">BMediaRoster</A> calls to issue requests to the various nodes available on the BeOS system on which your application is running.  In addition, you can't derive a new class directly from BMediaNode; instead, derive from one of the system-defined subclasses (BBufferConsumer, BBufferProducer, <A HREF="Controllable.html#BControllable">BControllable</A>, and so forth).
<P>
						</FONT>			
			</TABLE>
			<HR NOSHADE>
</TABLE>

<P>
Media Kit error code constants can be found in <B>MediaDefs.h</B>.
<P>
<HR>
<H3>
<A NAME="The%20Media%20Roster"></A>The Media Roster
</H3>
<P>
The Media Roster manages an application's communication with the media system.  Each application has at most one instance of the media roster.  The roster is obtained by calling <B><TT><A HREF="MediaRoster.html#Roster()">BMediaRoster::Roster()</A></TT></B>; if it already exists, the current roster object is returned.
<P>
This section briefly summarizes some of the functions served by the Media Roster; for more detailed information, see the <A HREF="MediaRoster.html#BMediaRoster">BMediaRoster</A> class.
<P>
<H4>
<A NAME="Finding%20the%20Right%20Nodes"></A>Finding the Right Nodes
</H4>
<P>
There are several standard nodes, which the user configures using the Media preference application, plus the system mixer.  The <A HREF="MediaRoster.html#BMediaRoster">BMediaRoster</A> class provides convenience routines to quickly get references to these nodes, such as <B><TT><A HREF="MediaRoster.html#GetAudioMixer()">GetAudioMixer()</A></TT></B> and <B><TT><A HREF="MediaRoster.html#GetVideoOutput()">GetVideoOutput()</A></TT></B>.  See the <A HREF="MediaRoster.html#BMediaRoster">BMediaRoster</A> class for details.
<P>
If you need some other node, you can browse through the available nodes to find the one best-suited for your needs.  Nodes are created from <B>dormant nodes</B>, which live inside media add-ons.  Each dormant node is a reference to a <B>node flavor</B>, a structure that describes the nodes the dormant node can create.
<P>
Once you've identified the best node for your purposes, you negotiate and establish a connection to the node.  This is discussed in the <A HREF="MediaRoster.html#BMediaRoster">BMediaRoster</A> overview.
<P>
<H4>
<A NAME="Controlling%20Nodes"></A>Controlling Nodes
</H4>
<P>
Once your nodes are created and connected to each other, you can control them by using the <A HREF="MediaRoster.html#BMediaRoster">BMediaRoster</A> functions <B><TT><A HREF="MediaNode.html#Preroll()">Preroll()</A></TT></B>, <B><TT><A HREF="MediaNode.html#Seek()">Seek()</A></TT></B>, <B><TT><A HREF="MediaNode.html#Start()">Start()</A></TT></B>, and <B><TT><A HREF="MediaNode.html#Stop()">Stop()</A></TT></B>.  These let you move to specific points in the media file and start and stop playback or recording.
<P>
You can also set the nodes' time sources, run modes, and play rates.
<P>
<H4>
<A NAME="Displaying%20a%20User%20Interface"></A>Displaying a User Interface
</H4>
<P>
<A HREF="Controllable.html#BControllable">BControllable</A> nodes can present a user interface representing the aspects of itself that the user can configure.  Each of these configurable aspects are called a <B>parameter</B>.  The <A HREF="MediaRoster.html#BMediaRoster">BMediaRoster</A> provides functions that let you create a user interface for a node's parameters.  See <B><TT><A HREF="MediaRoster.html#StartControlPanel()">BMediaRoster::StartControlPanel()</A></TT></B> for the easiest way to do this.
<P>
<HR>
<H3>
<A NAME="Media%20Files"></A>Media Files
</H3>
<P>
The approved way to access files containing media data is via the <A HREF="MediaFile.html#BMediaFile">BMediaFile</A> and <A HREF="MediaTrack.html#BMediaTrack">BMediaTrack</A> classes.  If you're using a node-based playback or recording system, and you want to have easy access to media files, you can get access to the node used by the <A HREF="MediaFile.html#BMediaFile">BMediaFile</A> class by calling <B><TT><A HREF="MediaRoster.html#SniffRef()">BMediaRoster::SniffRef()</A></TT></B>.
<P>
See <a href="encode_sample.html#Reading%20and%20Writing%20Media%20Files">"Reading and Writing Media Files"</a> for an example of how to access media files the right way.
<P>
<HR>
<H3>
<A NAME="The%20Audio%20Mixer"></A>The Audio Mixer
</H3>
<P>
The audio mixer accepts as input audio data which it then mixes and outputs to the audio output device or devices the user has selected in the Audio preference application.  Your application can get a media_node referencing the audio mixer using the <B><TT><A HREF="MediaRoster.html#GetAudioMixer()">BMediaRoster::GetAudioMixer()</A></TT></B> function.  You can't intercept audio being output by the audio mixer; they go directly to the output device.
<P>
Buffers containing any standard raw audio format can be sent to the audio mixer; the mixer will convert the data into the appropriate format for playback.
<P>
The audio mixer is always running, and is slaved to the most appropriate time source.  You should never change its time source or start or stop the audio mixer (in other words, don't call the <A HREF="MediaRoster.html#BMediaRoster">BMediaRoster</A> calls  <B><TT><A HREF="MediaRoster.html#SetTimeSourceFor()">SetTimeSourceFor()</A>,</TT></B> <B><TT><A HREF="MediaNode.html#Start()">Start()</A></TT></B>, or <B><TT><A HREF="MediaNode.html#Stop()">Stop()</A></TT></B> on the audio mixer).
<P>
<HR>
<H3>
<A NAME="The%20Audio%20Input"></A>The Audio Input
</H3>
<P>
The audio input creates audio buffers from external sources, such as microphones or line in ports.  The physical hardware device from which the sound is input is configured by the user using the Audio preference application.
<P>
In the current implementation of the Media Kit, the audio input doesn't let you change the sampling rate.  This may change in the future.  To ensure that your application will continue to work in the future, don't assume that the current sampling rate will remain in effect; instead, you should look at the <B><TT><A HREF="misc_api.html#media_format">media_format</A></TT></B> structure in the <B><TT><A HREF="misc_api.html#media_output">media_output</A></TT></B> you're using for your connection to the audio input:
<P>
<PRE>&nbsp;&nbsp;&nbsp;if (input->format.media_raw_audio_format.frame_rate != MY_FRAME_RATE) {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/* it's the wrong frame rate */
&nbsp;&nbsp;&nbsp;}</PRE>
<P>
The audio input is exclusive: only one connection to it is allowed at a time.  If you need to receive buffers from the input from two consumers, you'll need to create a special node that receives audio buffers, then sends copies of them to all the consumers that are attached to it.
<P>
<HR>
<H3>
<A NAME="Audio%20Playback%20Made%20Easy"></A>Audio Playback Made Easy
</H3>
<P>
If all you want to do is play back raw audio (such as AIFF or WAVE files), the Media Kit provides the <A HREF="SoundPlayer.html#BSoundPlayer">BSoundPlayer</A> class to simplify this process.  <A HREF="SoundPlayer.html#BSoundPlayer">BSoundPlayer</A> hides the inner workings of the Media Kit from you to make your life simple.  See these two classes for more information; an example on how to play audio files is given in the <A HREF="SoundPlayer.html#BSoundPlayer">BSoundPlayer</A> class overview.
<P>
You might also want to consider the various sound playback classes provided by the Game Kit, such as <A HREF="../Release%20Notes/GameKit.html#BSimpleGameSound">BSimpleGameSound</A> and BFileGameSound.
<P>
If this is still too much for you, you can use the <B><TT><A HREF="functions.html#play_sound()">play_sound()</A></TT></B> global C function to play a sound file.  The <B><TT><A HREF="functions.html#stop_sound()">stop_sound()</A></TT></B> function can be used to stop a sound started using <B><TT><A HREF="functions.html#play_sound()">play_sound()</A></TT></B>, and <B><TT><A HREF="functions.html#wait_for_sound()">wait_for_sound()</A></TT></B> lets you block until the sound finishes playing.
<P>
<A NAME="Creating%20New%20Node%20Classes"></A>
<P>
<HR>
<H2>
<A NAME="Creating%20New%20Node%20Classes"></A><FONT SIZE=6>C</FONT>reating <FONT SIZE=6>N</FONT>ew <FONT SIZE=6>N</FONT>ode <FONT SIZE=6>C</FONT>lasses
</H2>
<P>
You can create your own nodes to perform different types of media processing.  Nodes can be provided in add-ons that the Media Kit can load dormant nodes from, or in the application itself.  This is discussed in detail in the sections on <A HREF="MediaNode.html#BMediaNode">BMediaNode</A>, <A HREF="BufferConsumer.html#BBufferConsumer">BBufferConsumer</A>, and <A HREF="BufferProducer.html#BBufferProducer.">BBufferProducer.</A>
<P>
<TABLE  CELLPADDING=4>
	<TR>
		<TD>&nbsp;
		<TD>
			<HR NOSHADE>
			<TABLE CELLPADDING=4>
				<TR>
					<TD VALIGN=TOP>
						<IMG SRC="../art/infoBullet.gif">
					<TD><FONT FACE="helvetica">
If your node uses multiple threads, make sure you thread-protect calls to other nodes (in particular, be sure you thread-protect calls to the BTimeSource).  Use a semaphore or other appropriate protection mechanism.
<P>
						</FONT>			
			</TABLE>
			<HR NOSHADE>
</TABLE>

<P>
As a general rule, you should use the <A HREF="MediaEventLooper.html#BMediaEventLooper">BMediaEventLooper</A> class to handle the low-level scheduling and queuing of media events.  See <a href="ExampleNode.html#A%20BMediaEventLooper%20Example">"A BMediaEventLooper Example"</a> for an example of how this is done, including an explanation of the key points of creating a new media node.
<P>
<HR>
<H3>
<A NAME="Creating%20a%20Media%20Add-on"></A>Creating a Media Add-on
</H3>
<P>
This is discussed in detail in the <A HREF="MediaAddOn.html#BMediaAddOn">BMediaAddOn</A> class overview.
<P>
<HR>
<H3>
<A NAME="Application-based%20Nodes"></A>Application-based Nodes
</H3>
<P>
You can create your own node subclasses in an application if your application has special needs; just derive from the appropriate base class (such as BBufferConsumer) as normal.  Note, however, that your application should never directly call any of your subclass' functions; instead, you should register the node with the media roster, and control it via <A HREF="MediaRoster.html#BMediaRoster">BMediaRoster</A> calls, just like any other node, by using the media_node that describes your node.
<P>
Once you've written the code for your node class, you can register it with the Media Server by calling <B><TT><A HREF="MediaRoster.html#RegisterNode()">BMediaRoster::RegisterNode()</A></TT></B>.  When you're done with the node, you need to unregister it by calling <B><TT><A HREF="MediaRoster.html#UnregisterNode()">BMediaRoster::UnregisterNode()</A></TT></B>.  The easiest way to do this is just have the node class unregister itself when it's deleted.
<P>
<HR>
<H3>
<A NAME="Timing%20Issues"></A>Timing Issues
</H3>
<P>
When dealing with a number of nodes cooperating in processing data, there are always important timing concerns.  This section covers how various types of nodes need to behave in order to maintain proper timing.
<P>
<H4>
<A NAME="Calculating%20Buffer%20Start%20Times"></A>Calculating Buffer Start Times
</H4>
<P>
To calculate the presentation time at which a buffer should be performed, you should keep track of how many frames have been played, then multiply that value by 1000000LL/sample_rate (and, if your calculation is being done using floating-point math, you should <B><TT>floor()</TT></B> the result).  You can then apply whatever offset you want to <B><TT><A HREF="MediaNode.html#Seek()">Seek()</A></TT></B> to.
<P>
<PRE>&nbsp;&nbsp;&nbsp;buf->Header()->size_used = your_buf_frames * your_frame_size;
&nbsp;&nbsp;&nbsp;buf->Header->start_time = your_total_frames*1000000LL/your_format.frame_rate;
&nbsp;&nbsp;&nbsp;your_total_frames += your_buf_frames;</PRE>
<P>
You shouldn't compute the start time by adding the previous buffer's duration to its start time; the accumulation of rounding errors over time will cause dropped samples about three times per second if you do.
<P>
<H4>
<A NAME="Producers"></A>Producers
</H4>
<P>
Producers that produce buffers intended for output need to stamp each buffer it creates with a <I><FONT  color=991122 face=HELVETICA>startTime</I></FONT>, which indicates the performance time at which the buffer should be played.  If the producer is playing media from a file, or synchronizing sound, this is the time at which the media should become analog.
<P>
In order to compute this <I><FONT  color=991122 face=HELVETICA>startTime</I></FONT> properly, the producer must prepare the buffers in advance, by the amount of time reported by <B><TT><A HREF="BufferProducer.html#FindLatencyFor()">BBufferProducer::FindLatencyFor()</A></TT></B>.  The producer also needs to respond to the <A HREF="BufferProducer.html#LateNoticeReceived()">BBufferProducer::LateNoticeReceived()</A> hook function by at least updating the time stamps it's putting on the buffers it's sending out, so they'll be played by the downstream nodes, which should be checking those times to play them at the correct time (and may be dropping buffers if they're late).  If this isn't done, things will tend to get further and further behind.
<P>
<TABLE  CELLPADDING=4>
	<TR>
		<TD>&nbsp;
		<TD>
			<HR NOSHADE>
			<TABLE CELLPADDING=4>
				<TR>
					<TD VALIGN=TOP>
						<IMG SRC="../art/infoBullet.gif">
					<TD><FONT FACE="helvetica">
In general, it's best to try to produce buffers as late as possible without actually causing them to arrive at their destination late (ie, they should be sent at or before the time <I><FONT  color=991122 face=HELVETICA>presentationTime</I></FONT> - <I><FONT  color=991122 face=HELVETICA>downstreamLatency</I></FONT>). This will ensure the best overall performance by reducing the number of buffers that are pending (especially if the user starts playing with time such that your node gets seeked or stopped).  Also, if you're producing buffers that have a real-world connection, such as to a video display, producing them too early might cause them to be displayed early.
<P>
						</FONT>			
			</TABLE>
			<HR NOSHADE>
</TABLE>

<P>
If a producer is producing buffers that are being generated by a physical input (such as a microphone jack, for example) handle things somewhat differently.  They stamp the generated buffers with the performance time at which they were captured (this should be the time at which the first sample in the buffer was taken).  This means that when these buffers are transmitted downstream, they'll always be "late" in the eyes of any node they arrive at.
<P>
This also means you can't easily hook a physical input to a physical output, because buffers will always arrive at the output later than the timestamped value.  You need to insert another node between the two to adjust the time stamps appropriately so they won't be "late" anymore.
<P>
Additionally, nodes that record data (such as file-writing nodes), in the <B><TT><A HREF="MediaNode.html#B_RECORDING">B_RECORDING</A></TT></B> run mode, shouldn't care about buffers that arrive late; this lets data be recorded without concern for this issue.
<P>
<H4>
<A NAME="Consumers"></A>Consumers
</H4>
<P>
If the consumer is the device that recognizes the media (ie, it plays the audio or video contained in the buffers it receives), it needs to report the correct latency back to the producer for the time it takes buffers to reach the analog world (ie, the amount of time it takes to present the data in the buffer to the user, whether it's audio or video).  Buffers that are received shouldn't be played until the <I><FONT  color=991122 face=HELVETICA>startTime</I></FONT> stamped on the buffers arrives.  If buffers arrive late, the consumer should send a late notice to the producer, so it can make the necessary adjustments, and not pass the buffer along at all; be sure to <B><TT><A HREF="Buffer.html#Recycle()">Recycle()</A></TT></B> the buffers so they can be reused.
<P>
<H4>
<A NAME="Consumer/Producers%20(Filters)"></A>Consumer/Producers (Filters)
</H4>
<P>
A consumer/producer (filter) must report the correct latency for the time a buffer takes to pass through the filter from the time it's received to the time it's retransmitted, plus the downstream latency.  It shouldn't change the time stamp, unless this explicitly part of the filter's purpose.  The filter should also handle late packets as described under Producers and Consumers above.
<P>
<H4>
<A NAME="Media%20Applications"></A>Media Applications
</H4>
<P>
The application that starts the nodes and the time source to which they're slaved needs to provide them with the correct starting times.  For example, if several nodes have been connected, they've all been slaved to an appropriate time source, and you want to start them all up, you need to take the following steps:
<P>
<UL>
<LI>Find the latency of the entire network, so you can give it time to come up to speed.
<P>
<LI>Start all the nodes with the performance time at which they should start playing.
<P>
<LI>Seek the time source to the performance time and real time that you want the performance time to be related to.  This is crucial: it establishes the relationship between performance time and real time; if you forget to do this, things will look and sound very unpleasant as the media tries desperately to adjust to the actual time.
<P>
</UL>
<PRE>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bigtime_t latency;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Roster->GetLatencyFor(node1, &amp;latency);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Roster->PrerollNode(node1);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Roster->PrerollNode(node2);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Roster->PrerollNode(node3);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Roster->StartNode(node1, 0);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Roster->StartNode(node2, 0);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Roster->StartNode(node3, 0);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bigtime_t now = system_time();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Roster->SeekNode(timesourceNode, -latency, now + 10000);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Roster->StartNode(timesourceNode, now + 10000);</PRE>
<P>
The extra 10,000 microseconds is added in case the code gets preempted while preparing to start the timesourceNode; this gives us a little fudge factor so we don't start out behind.
<P>
<H4>
<A NAME="B_OFFLINE%20Run%20Mode"></A>B_OFFLINE Run Mode
</H4>
<P>
Nodes that run in offline mode are a special case in the timing world.
<P>
<H5>
<A NAME="Consumers"></A>Consumers
</H5>
<P>
in <B><TT><A HREF="MediaNode.html#B_OFFLINE">B_OFFLINE</A></TT></B> mode derive their current time from the arrival of buffers on their inputs.  The current performance time is the minumum of all times received on its active inputs.  Active inputs are those inputs that are connected and haven't received a <B><TT><A HREF="BufferConsumer.html#ProducerDataStatus()">ProducerDataStatus()</A></TT></B> call indicating that there are no buffers coming.  You receive a time when you get a buffer <B><TT>start_time</TT></B> or a <B><TT><A HREF="BufferConsumer.html#ProducerDataStatus()">ProducerDataStatus()</A></TT></B> call with <B><TT>B_PRODUCER_STOPPED</TT></B>.
<P>
Consumers in offline mode should call <B><TT><A HREF="BufferConsumer.html#RequestAdditionalBuffer()">RequestAdditionalBuffer()</A></TT></B> once it's received and processed a buffer on one of its inputs in order to obtain further buffers.
<P>
<H5>
<A NAME="Producers"></A>Producers
</H5>
<P>
just send buffers in sequence in <B><TT><A HREF="MediaNode.html#B_OFFLINE">B_OFFLINE</A></TT></B> mode.  The recommended behavior is to send the first buffer, then wait for an <B><TT><A HREF="BufferProducer.html#AdditionalBufferRequested()">AdditionalBufferRequested()</A></TT></B> call before sending the next buffer.  If this request doesn't arrive within a reasonable amount of time (a second or so, depending on your application), your node should accept that it's working with a not-so-bright consumer and start sending buffers at your convenience.
<P>
<TABLE  CELLPADDING=4>
	<TR>
		<TD>&nbsp;
		<TD>
			<HR NOSHADE>
			<TABLE CELLPADDING=4>
				<TR>
					<TD VALIGN=TOP>
						<IMG SRC="../art/warningBullet.gif">
					<TD><FONT FACE="helvetica">
Don't call the time source from your producer in <B><TT><A HREF="MediaNode.html#B_OFFLINE">B_OFFLINE</A></TT></B> mode.
<P>
						</FONT>			
			</TABLE>
			<HR NOSHADE>
</TABLE>

<P>
If a producer has ever received an <B><TT><A HREF="BufferProducer.html#AdditionalBufferRequested()">AdditionalBufferRequested()</A></TT></B> call, it should assume that the consumer knows what it's doing and should only send buffers on request.
<P>
<HR>
<H2>
<A NAME="Installing%20Media%20Nodes%20and%20Drivers"></A><FONT SIZE=6>I</FONT>nstalling <FONT SIZE=6>M</FONT>edia <FONT SIZE=6>N</FONT>odes and <FONT SIZE=6>D</FONT>rivers
</H2>
<P>
Media node add-ons should be installed in the <B>/boot/home/config/add-ons/media</B> directory.
<P>
Media drivers should be installed in <B>/boot/home/config/add-ons/kernel/drivers/bin</B>.  Then create a simlink to the driver in <B>/boot/home/config/add-ons/kernel/drivers/dev/(type)</B>, where (type) is the type of driver you're installing (audio, video, etc).
<P>
After installing a media node add-on, you have to restart the Media Server for it to become available for use.
<P>
<HR>
<H2>
<A NAME="About%20enum%20Members%20of%20Classes"></A><FONT SIZE=6>A</FONT>bout enum <FONT SIZE=6>M</FONT>embers of <FONT SIZE=6>C</FONT>lasses
</H2>
<P>
The Media Kit has several classes (most notably, BMediaNode) that contain, as members, enums.  For instance, in <A HREF="MediaNode.html#BMediaNode">BMediaNode</A>, you'll find the following:
<P>
<PRE>&nbsp;&nbsp;&nbsp;class BMediaNode {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;enum run_mode {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B_OFFLINE = 1,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B_DECREASE_PRECISION,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B_INCREASE_LATENCY,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B_DROP_DATA,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B_RECORDING
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;};
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...
&nbsp;&nbsp;&nbsp;};</PRE>
<P>
In this case, you can freely use <B><TT><A HREF="MediaNode.html#B_OFFLINE">B_OFFLINE</A></TT></B> and so forth from within objects derived from <A HREF="MediaNode.html#BMediaNode">BMediaNode</A>, but if you want to use these values from other classes (or outside any class), you need to use the notation <B><TT><A HREF="MediaNode.html#B_OFFLINE">BMediaNode::B_OFFLINE</A></TT></B> to use these constants.  This is true of any enum defined within a class; this will be called-out specifically in the descriptions of any constants in this chapter.
<P>
<A NAME="About%20Multiple%20Virtual%20Inheritance"></A>
<P>
<HR>
<H2>
<A NAME="About%20Multiple%20Virtual%20Inheritance"></A><FONT SIZE=6>A</FONT>bout <FONT SIZE=6>M</FONT>ultiple <FONT SIZE=6>V</FONT>irtual <FONT SIZE=6>I</FONT>nheritance
</H2>
<P>
Virtual inheritance is slightly different from regular inheritance in C++.  The constructor for the virtual base class has to be explicitly (or implicitly) called from the most-derived class being instantiated, rather than being called from the direct descendant class actually defining the virtual inheritance.
<P>
In simple terms, this means that whenever you derive a new class from a class that uses virtual inheritance, your derived class's constructor should explicitly call the parent class's constructor.
<P>
</PRE></TABLE></UL></MENU></B></I></TT>



<!--TOP LINKS-->
<HR>
<center>
<TABLE bgcolor=ddeeee>
<TR><TD>
<A HREF="index.html"><FONT face=HELVETICA>The Media Kit Table of Contents</FONT></A>&nbsp;&nbsp;
<TD>
&nbsp;&nbsp;<A HREF="The%20Media%20Kit%20Master%20Index.html"><FONT face=HELVETICA>The Media Kit Index</FONT></A>
</TABLE></center>
<!--TOP LINKS-->


<!-- Footer for Release 5 HTML Be Book -->
<hr>
<br>
<p>
<center>
<i><FONT size=6>T</FONT>he <FONT size=6 color=blue>B</FONT><FONT size=6 color=red>e</font> <FONT size=6>B</FONT>ook</i>,
<br>...in lovely HTML...
<br>for BeOS Release 5.
<center>
<br>
<p><font face=helvetiva>Copyright &copy; 2000 Be, Inc.  All rights reserved..
</body>
</html>

